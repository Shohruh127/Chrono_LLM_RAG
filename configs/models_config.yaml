# =============================================================================
# configs/models_config.yaml - Tri-Force Model Stack Configuration
# Created by: Shohruh127
# Repository: Shohruh127/Chrono_LLM_RAG
# =============================================================================

# VRAM Management
vram_budget_gb: 30.0  # Maximum VRAM usage (A100 has 40-80GB)

# ============================================================================
# Forecaster: Chronos-T5-Base
# Purpose: Zero-shot time series forecasting for small economic data
# Estimated VRAM: ~2GB (base model, no quantization needed)
# ============================================================================
forecaster:
  model_id: "amazon/chronos-t5-base"
  use_quantization: false  # Chronos models use different loading mechanism
  max_context_length: 512
  prediction_length: 4
  quantile_levels: [0.1, 0.5, 0.9]
  num_samples: 20
  dtype: "bfloat16"

# ============================================================================
# Logic Engineer: Qwen2.5-Coder-7B
# Purpose: Python code generation and logic reasoning
# Estimated VRAM: ~4.5GB with NF4 quantization
# ============================================================================
logic_engineer:
  model_id: "Qwen/Qwen2.5-Coder-7B"
  use_quantization: true
  quantization_type: "nf4"
  compute_dtype: "float16"
  max_new_tokens: 1024
  temperature: 0.3
  top_p: 0.9
  repetition_penalty: 1.1
  max_context_length: 4096

# ============================================================================
# Cultural Analyst: Llama-3.1-8B-Uz
# Purpose: Uzbek linguistic nuance and tone analysis
# Estimated VRAM: ~5GB with NF4 quantization
# ============================================================================
cultural_analyst:
  model_id: "behbudiy/Llama-3.1-8B-Uz"
  use_quantization: true
  quantization_type: "nf4"
  compute_dtype: "float16"
  max_new_tokens: 800
  temperature: 0.5
  top_p: 0.85
  repetition_penalty: 1.3
  max_context_length: 4096

# ============================================================================
# Quantization Settings (bitsandbytes)
# ============================================================================
quantization:
  # NF4 (NormalFloat4) - Most memory efficient
  nf4:
    load_in_4bit: true
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true
    bnb_4bit_compute_dtype: "float16"
  
  # INT8 - Faster but uses more memory
  int8:
    load_in_8bit: true
    llm_int8_threshold: 6.0

# ============================================================================
# Query Routing Keywords
# ============================================================================
routing:
  forecast_keywords:
    - "forecast"
    - "predict"
    - "bashorat"
    - "prognoz"
    - "trend"
    - "future"
    - "projection"
    - "estimate"
    - "time series"
    - "vaqt qatori"
  
  code_keywords:
    - "code"
    - "python"
    - "calculate"
    - "compute"
    - "function"
    - "algorithm"
    - "script"
    - "program"
    - "formula"
    - "hisoblash"
    - "kod"
  
  cultural_keywords:
    - "uzbek"
    - "o'zbek"
    - "toshkent"
    - "uzbekistan"
    - "madaniyat"
    - "til"
    - "language"
    - "cultural"
    - "translate"
    - "tarjima"
    - "matn"

# ============================================================================
# Hardware Detection
# ============================================================================
hardware:
  preferred_device: "cuda"  # cuda, cpu, or auto
  fallback_to_cpu: true
  cache_dir: "models/cache"
