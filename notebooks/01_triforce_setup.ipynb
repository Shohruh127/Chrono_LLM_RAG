{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Tri-Force Model Stack Setup\n",
        "\n",
        "**Phase 1: Sovereign Runtime Environment on Google Colab Pro (A100)**\n",
        "\n",
        "This notebook demonstrates hot-path inference with three specialist models:\n",
        "\n",
        "1. **Forecaster**: `amazon/chronos-t5-base` - Zero-shot time series forecasting\n",
        "2. **Logic Engineer**: `Qwen/Qwen2.5-Coder-7B` - Python code generation\n",
        "3. **Cultural Analyst**: `behbudiy/Llama-3.1-8B-Uz` - Uzbek linguistic analysis\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Google Colab Pro with A100 GPU\n",
        "- ~30GB VRAM available\n",
        "- High-RAM runtime enabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository (skip if already cloned)\n",
        "!git clone https://github.com/Shohruh127/Chrono_LLM_RAG.git 2>/dev/null || echo 'Repository already exists'\n",
        "%cd Chrono_LLM_RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "# Verify bitsandbytes installation\n",
        "!python -c \"import bitsandbytes; print(f'bitsandbytes version: {bitsandbytes.__version__}')\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected! Enable GPU in Runtime > Change runtime type\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hardware Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.tri_force import HardwareOptimizer, check_gpu\n",
        "\n",
        "# Initialize hardware optimizer\n",
        "optimizer = HardwareOptimizer(vram_budget_gb=30.0)\n",
        "\n",
        "print(f\"Device: {optimizer.device}\")\n",
        "print(f\"Device name: {optimizer.device_name}\")\n",
        "\n",
        "# Get initial VRAM report\n",
        "optimizer.print_vram_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show quantization configuration\n",
        "quant_config = optimizer.get_quantization_config()\n",
        "print(\"NF4 Quantization Config:\")\n",
        "for key, value in quant_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Estimate VRAM requirements\n",
        "print(\"\\nüìä Estimated VRAM Requirements:\")\n",
        "print(f\"  Chronos-T5-Base (~0.2B params): {optimizer.get_model_memory_estimate(0.2, 'fp16'):.2f} GB\")\n",
        "print(f\"  Qwen2.5-Coder-7B (NF4): {optimizer.get_model_memory_estimate(7, 'nf4'):.2f} GB\")\n",
        "print(f\"  Llama-3.1-8B-Uz (NF4): {optimizer.get_model_memory_estimate(8, 'nf4'):.2f} GB\")\n",
        "total_est = optimizer.get_model_memory_estimate(0.2, 'fp16') + optimizer.get_model_memory_estimate(7, 'nf4') + optimizer.get_model_memory_estimate(8, 'nf4')\n",
        "print(f\"  Total Estimated: {total_est:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Tri-Force Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.tri_force import TriForceStack, QueryType\n",
        "\n",
        "# Initialize stack with configuration\n",
        "stack = TriForceStack(config_path=\"configs/models_config.yaml\")\n",
        "\n",
        "print(\"Tri-Force Stack initialized\")\n",
        "print(f\"Device: {stack.hardware.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Benchmark: Sequential Loading vs Hot-Path Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Clear cache before benchmark\n",
        "optimizer.clear_cache()\n",
        "\n",
        "# Sequential Loading Benchmark\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä BENCHMARK: Sequential Loading\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sequential_times = []\n",
        "\n",
        "# Load and unload each model sequentially\n",
        "start = time.time()\n",
        "stack.load_forecaster()\n",
        "sequential_times.append((\"Forecaster\", time.time() - start))\n",
        "stack._forecaster = None\n",
        "stack._models_loaded[\"forecaster\"] = False\n",
        "optimizer.clear_cache()\n",
        "\n",
        "start = time.time()\n",
        "stack.load_logic_engineer()\n",
        "sequential_times.append((\"Logic Engineer\", time.time() - start))\n",
        "stack._logic_engineer = None\n",
        "stack._models_loaded[\"logic_engineer\"] = False\n",
        "optimizer.clear_cache()\n",
        "\n",
        "start = time.time()\n",
        "stack.load_cultural_analyst()\n",
        "sequential_times.append((\"Cultural Analyst\", time.time() - start))\n",
        "stack._cultural_analyst = None\n",
        "stack._models_loaded[\"cultural_analyst\"] = False\n",
        "optimizer.clear_cache()\n",
        "\n",
        "total_sequential = sum(t for _, t in sequential_times)\n",
        "print(f\"\\nSequential load times:\")\n",
        "for name, t in sequential_times:\n",
        "    print(f\"  {name}: {t:.2f}s\")\n",
        "print(f\"Total sequential: {total_sequential:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hot-Path Loading Benchmark\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ BENCHMARK: Hot-Path Inference (All Models Loaded)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start = time.time()\n",
        "stack.load_all()\n",
        "hot_path_time = time.time() - start\n",
        "\n",
        "print(f\"\\nHot-path load time: {hot_path_time:.2f}s\")\n",
        "\n",
        "# Calculate improvement\n",
        "improvement = ((total_sequential - hot_path_time) / total_sequential) * 100 if total_sequential > 0 else 0\n",
        "print(f\"\\nüìà Latency improvement: {improvement:.1f}%\")\n",
        "print(f\"   (Sequential: {total_sequential:.2f}s vs Hot-path: {hot_path_time:.2f}s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. VRAM Usage Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get final VRAM report\n",
        "report = optimizer.get_vram_usage()\n",
        "\n",
        "# Create VRAM visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Pie chart: VRAM usage\n",
        "if report.total_gb > 0:\n",
        "    sizes = [report.used_gb, report.free_gb]\n",
        "    labels = [f'Used\\n{report.used_gb:.1f} GB', f'Free\\n{report.free_gb:.1f} GB']\n",
        "    colors = ['#ff6b6b', '#4ecdc4']\n",
        "    explode = (0.05, 0)\n",
        "    \n",
        "    axes[0].pie(sizes, explode=explode, labels=labels, colors=colors, \n",
        "                autopct='%1.1f%%', shadow=True, startangle=90)\n",
        "    axes[0].set_title(f'VRAM Usage on {report.device_name}\\n(Total: {report.total_gb:.1f} GB)')\n",
        "\n",
        "# Bar chart: Model VRAM estimates\n",
        "models = ['Chronos-T5\\n(FP16)', 'Qwen2.5-7B\\n(NF4)', 'Llama-3.1-8B\\n(NF4)', 'Total\\nEstimate', 'Actual\\nUsage', 'Budget']\n",
        "vram_values = [\n",
        "    optimizer.get_model_memory_estimate(0.2, 'fp16'),\n",
        "    optimizer.get_model_memory_estimate(7, 'nf4'),\n",
        "    optimizer.get_model_memory_estimate(8, 'nf4'),\n",
        "    total_est,\n",
        "    report.used_gb,\n",
        "    optimizer.vram_budget_gb\n",
        "]\n",
        "colors = ['#3498db', '#9b59b6', '#e74c3c', '#2ecc71', '#f39c12', '#95a5a6']\n",
        "\n",
        "bars = axes[1].bar(models, vram_values, color=colors)\n",
        "axes[1].set_ylabel('VRAM (GB)')\n",
        "axes[1].set_title('VRAM Allocation by Model')\n",
        "axes[1].axhline(y=optimizer.vram_budget_gb, color='red', linestyle='--', label='Budget')\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, vram_values):\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
        "                 f'{val:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('vram_usage.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ VRAM usage saved to vram_usage.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Health Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Run health check\n",
        "health = stack.health_check()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üè• Model Health Check\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(json.dumps(health, indent=2, default=str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Query Routing Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test query routing\n",
        "test_queries = [\n",
        "    \"Forecast GDP growth for the next 4 years\",\n",
        "    \"Write Python code to calculate compound interest\",\n",
        "    \"O'zbekiston iqtisodiyoti haqida gapirib bering\",\n",
        "    \"What is the trend in agricultural output?\",\n",
        "    \"Calculate the mean of these values\",\n",
        "    \"Toshkent viloyatining sanoat ko'rsatkichlari\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üîÄ Query Routing Test\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for query in test_queries:\n",
        "    query_type = stack.detect_query_type(query)\n",
        "    print(f\"\\nüìù Query: '{query[:50]}...'\")\n",
        "    print(f\"   ‚Üí Type: {query_type.value}\")\n",
        "    print(f\"   ‚Üí Model: {['forecaster', 'logic_engineer', 'cultural_analyst'][['FORECAST', 'CODE', 'CULTURAL'].index(query_type.name) if query_type.name in ['FORECAST', 'CODE', 'CULTURAL'] else 2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Example Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test forecasting with sample data\n",
        "import torch\n",
        "\n",
        "# Sample time series data (e.g., annual GDP values)\n",
        "sample_context = torch.tensor([100.0, 105.0, 110.0, 115.0, 120.0, 125.0, 130.0, 135.0])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà Forecasting Example\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nHistorical data: {sample_context.tolist()}\")\n",
        "\n",
        "# Generate forecasts\n",
        "forecasts = stack.forecast(\n",
        "    context=sample_context,\n",
        "    prediction_length=4,\n",
        "    num_samples=20\n",
        ")\n",
        "\n",
        "print(f\"\\nForecast shape: {forecasts.shape}\")\n",
        "print(f\"Forecast mean: {forecasts.mean(dim=1).squeeze().tolist()}\")\n",
        "print(f\"Forecast 10th percentile: {forecasts.quantile(0.1, dim=1).squeeze().tolist()}\")\n",
        "print(f\"Forecast 90th percentile: {forecasts.quantile(0.9, dim=1).squeeze().tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test code generation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üíª Code Generation Example\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "code_query = \"Write a Python function to calculate moving average of a list\"\n",
        "result = stack.route_query(code_query, QueryType.CODE)\n",
        "\n",
        "print(f\"\\nQuery: {code_query}\")\n",
        "print(f\"Model: {result['model']}\")\n",
        "print(f\"\\nResponse:\\n{result['response'][:500]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test cultural analysis\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üåç Cultural Analysis Example\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cultural_query = \"Toshkent viloyatining iqtisodiy rivojlanishi haqida qisqacha ma'lumot bering\"\n",
        "result = stack.route_query(cultural_query, QueryType.CULTURAL)\n",
        "\n",
        "print(f\"\\nQuery: {cultural_query}\")\n",
        "print(f\"Model: {result['model']}\")\n",
        "print(f\"\\nResponse:\\n{result['response'][:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Final VRAM Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final VRAM report\n",
        "optimizer.print_vram_report()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Tri-Force Stack Setup Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"   - All 3 models loaded: {stack.health_check()['all_loaded']}\")\n",
        "print(f\"   - VRAM within budget: {optimizer.check_vram_budget()}\")\n",
        "print(f\"   - Device: {optimizer.device_name}\")\n",
        "print(f\"   - Hot-path inference ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cleanup (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unload all models (optional - for memory management)\n",
        "# stack.unload_all()\n",
        "# optimizer.print_vram_report()"
      ]
    }
  ]
}
